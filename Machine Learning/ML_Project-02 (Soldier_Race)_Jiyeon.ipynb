{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfUdHCfe1b6v"
   },
   "source": [
    "___\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "heading_collapsed": true,
    "id": "CvFxPmf41b6y"
   },
   "source": [
    "# WELCOME!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "id": "WZUMKNQY1b6y"
   },
   "source": [
    "In this project, you must apply EDA processes for the development of predictive models. Handling outliers, domain knowledge and feature engineering will be challenges.\n",
    "\n",
    "Also, this project aims to improve your ability to implement algorithms for Multi-Class Classification. Thus, you will have the opportunity to implement many algorithms commonly used for Multi-Class Classification problems.\n",
    "\n",
    "Before diving into the project, please take a look at the determines and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "heading_collapsed": true,
    "id": "laCRtJs51b6z"
   },
   "source": [
    "# Determines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "id": "iixh9Bej1b6z"
   },
   "source": [
    "The 2012 US Army Anthropometric Survey (ANSUR II) was executed by the Natick Soldier Research, Development and Engineering Center (NSRDEC) from October 2010 to April 2012 and is comprised of personnel representing the total US Army force to include the US Army Active Duty, Reserves, and National Guard. In addition to the anthropometric and demographic data described below, the ANSUR II database also consists of 3D whole body, foot, and head scans of Soldier participants. These 3D data are not publicly available out of respect for the privacy of ANSUR II participants. The data from this survey are used for a wide range of equipment design, sizing, and tariffing applications within the military and has many potential commercial, industrial, and academic applications.\n",
    "\n",
    "The ANSUR II working databases contain 93 anthropometric measurements which were directly measured, and 15 demographic/administrative variables explained below. The ANSUR II Male working database contains a total sample of 4,082 subjects. The ANSUR II Female working database contains a total sample of 1,986 subjects.\n",
    "\n",
    "\n",
    "DATA DICT:\n",
    "https://data.world/datamil/ansur-ii-data-dictionary/workspace/file?filename=ANSUR+II+Databases+Overview.pdf\n",
    "\n",
    "---\n",
    "\n",
    "To achieve high prediction success, you must understand the data well and develop different approaches that can affect the dependent variable.\n",
    "\n",
    "Firstly, try to understand the dataset column by column using pandas module. Do research within the scope of domain (body scales, and race characteristics) knowledge on the internet to get to know the data set in the fastest way. \n",
    "\n",
    "You will implement ***Logistic Regression, Support Vector Machine, XGBoost, Random Forest*** algorithms. Also, evaluate the success of your models with appropriate performance metrics.\n",
    "\n",
    "At the end of the project, choose the most successful model and try to enhance the scores with ***SMOTE*** make it ready to deploy. Furthermore, use ***SHAP*** to explain how the best model you choose works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "heading_collapsed": true,
    "id": "P2UckCvP1b60"
   },
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "hidden": true,
    "id": "gCVDEsGB1b60"
   },
   "source": [
    "#### 1. Exploratory Data Analysis (EDA)\n",
    "- Import Libraries, Load Dataset, Exploring Data\n",
    "\n",
    "    *i. Import Libraries*\n",
    "    \n",
    "    *ii. Ingest Data *\n",
    "    \n",
    "    *iii. Explore Data*\n",
    "    \n",
    "    *iv. Outlier Detection*\n",
    "    \n",
    "    *v.  Drop unnecessary features*\n",
    "\n",
    "#### 2. Data Preprocessing\n",
    "- Scale (if needed)\n",
    "- Separete the data frame for evaluation purposes\n",
    "\n",
    "#### 3. Multi-class Classification\n",
    "- Import libraries\n",
    "- Implement SVM Classifer\n",
    "- Implement Decision Tree Classifier\n",
    "- Implement Random Forest Classifer\n",
    "- Implement XGBoost Classifer\n",
    "- Compare The Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "Y5-Kay1Eqvdj"
   },
   "source": [
    "# EDA\n",
    "- Drop unnecessary colums\n",
    "- Drop DODRace class if value count below 500 (we assume that our data model can't learn if it is below 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "K7UZHtvu1b62"
   },
   "source": [
    "## 1. Import Libraries\n",
    "Besides Numpy and Pandas, you need to import the necessary modules for data visualization, data preprocessing, Model building and tuning.\n",
    "\n",
    "*Note: Check out the course materials.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.1\n",
      "  latest version: 23.7.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.7.2\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/songjiyeon/opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - xgboost\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    _py-xgboost-mutex-2.0      |            cpu_0           8 KB  conda-forge\n",
      "    conda-23.7.2               |   py39h6e9494a_0         995 KB  conda-forge\n",
      "    libblas-3.9.0              |17_osx64_openblas          14 KB  conda-forge\n",
      "    libcblas-3.9.0             |17_osx64_openblas          14 KB  conda-forge\n",
      "    libgfortran-5.0.0          |12_3_0_h97931a8_1         130 KB  conda-forge\n",
      "    libgfortran5-12.3.0        |       hbd3c1fe_1         1.5 MB  conda-forge\n",
      "    liblapack-3.9.0            |17_osx64_openblas          14 KB  conda-forge\n",
      "    libopenblas-0.3.23         |openmp_h429af6e_0         5.7 MB  conda-forge\n",
      "    libxgboost-1.7.4           |   cpu_h0c1cf5f_0         1.5 MB  conda-forge\n",
      "    notebook-6.3.0             |   py39h6e9494a_0         6.3 MB  conda-forge\n",
      "    numpy-1.23.5               |   py39hdfa1d0c_0         5.3 MB  conda-forge\n",
      "    pooch-1.7.0                |     pyha770c72_3          50 KB  conda-forge\n",
      "    py-xgboost-1.7.4           |cpu_py39hb0a6171_0         206 KB  conda-forge\n",
      "    python_abi-3.9             |           2_cp39           4 KB  conda-forge\n",
      "    scikit-learn-1.2.2         |   py39h151e6e4_1         6.6 MB  conda-forge\n",
      "    scipy-1.10.1               |   py39h4c5e66d_2        19.5 MB  conda-forge\n",
      "    threadpoolctl-3.2.0        |     pyha21a80b_0          20 KB  conda-forge\n",
      "    xgboost-1.7.4              |cpu_py39hd182d69_0          14 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        47.9 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  _py-xgboost-mutex  conda-forge/osx-64::_py-xgboost-mutex-2.0-cpu_0 \n",
      "  joblib             conda-forge/noarch::joblib-1.3.0-pyhd8ed1ab_1 \n",
      "  libblas            conda-forge/osx-64::libblas-3.9.0-17_osx64_openblas \n",
      "  libcblas           conda-forge/osx-64::libcblas-3.9.0-17_osx64_openblas \n",
      "  libgfortran        conda-forge/osx-64::libgfortran-5.0.0-12_3_0_h97931a8_1 \n",
      "  libgfortran5       conda-forge/osx-64::libgfortran5-12.3.0-hbd3c1fe_1 \n",
      "  liblapack          conda-forge/osx-64::liblapack-3.9.0-17_osx64_openblas \n",
      "  libopenblas        conda-forge/osx-64::libopenblas-0.3.23-openmp_h429af6e_0 \n",
      "  libxgboost         conda-forge/osx-64::libxgboost-1.7.4-cpu_h0c1cf5f_0 \n",
      "  numpy              conda-forge/osx-64::numpy-1.23.5-py39hdfa1d0c_0 \n",
      "  pooch              conda-forge/noarch::pooch-1.7.0-pyha770c72_3 \n",
      "  py-xgboost         conda-forge/osx-64::py-xgboost-1.7.4-cpu_py39hb0a6171_0 \n",
      "  python_abi         conda-forge/osx-64::python_abi-3.9-2_cp39 \n",
      "  scikit-learn       conda-forge/osx-64::scikit-learn-1.2.2-py39h151e6e4_1 \n",
      "  scipy              conda-forge/osx-64::scipy-1.10.1-py39h4c5e66d_2 \n",
      "  threadpoolctl      conda-forge/noarch::threadpoolctl-3.2.0-pyha21a80b_0 \n",
      "  xgboost            conda-forge/osx-64::xgboost-1.7.4-cpu_py39hd182d69_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2023.05.30~ --> conda-forge::ca-certificates-2023.7.22-h8857fd0_0 \n",
      "  conda              pkgs/main::conda-23.7.1-py39hecd8cb5_0 --> conda-forge::conda-23.7.2-py39h6e9494a_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/osx-64::certifi-2023.7.22-p~ --> conda-forge/noarch::certifi-2023.7.22-pyhd8ed1ab_0 \n",
      "  notebook           pkgs/main::notebook-6.5.4-py39hecd8cb~ --> conda-forge::notebook-6.3.0-py39h6e9494a_0 \n",
      "  openssl              pkgs/main::openssl-1.1.1u-hca72f7f_0 --> conda-forge::openssl-1.1.1u-h8a1eda9_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "py-xgboost-1.7.4     | 206 KB    |                                       |   0% \n",
      "scipy-1.10.1         | 19.5 MB   |                                       |   0% \u001b[A\n",
      "\n",
      "liblapack-3.9.0      | 14 KB     |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libblas-3.9.0        | 14 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libgfortran5-12.3.0  | 1.5 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libxgboost-1.7.4     | 1.5 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scikit-learn-1.2.2   | 6.6 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pooch-1.7.0          | 50 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "xgboost-1.7.4        | 14 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libcblas-3.9.0       | 14 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "conda-23.7.2         | 995 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python_abi-3.9       | 4 KB      |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "threadpoolctl-3.2.0  | 20 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran-5.0.0    | 130 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.23   | 5.7 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_py-xgboost-mutex-2. | 8 KB      |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "numpy-1.23.5         | 5.3 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "notebook-6.3.0       | 6.3 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libgfortran5-12.3.0  | 1.5 MB    | 3                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libblas-3.9.0        | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "liblapack-3.9.0      | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\n",
      "scipy-1.10.1         | 19.5 MB   |                                       |   0% \u001b[A\n",
      "\n",
      "\n",
      "libblas-3.9.0        | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "py-xgboost-1.7.4     | 206 KB    | ##8                                   |   8% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scikit-learn-1.2.2   | 6.6 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libgfortran5-12.3.0  | 1.5 MB    | ##################5                   |  50% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libxgboost-1.7.4     | 1.5 MB    | 3                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "scipy-1.10.1         | 19.5 MB   | #1                                    |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "py-xgboost-1.7.4     | 206 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libgfortran5-12.3.0  | 1.5 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scikit-learn-1.2.2   | 6.6 MB    | ####1                                 |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libxgboost-1.7.4     | 1.5 MB    | #######################2              |  63% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "xgboost-1.7.4        | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pooch-1.7.0          | 50 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "scipy-1.10.1         | 19.5 MB   | #######1                              |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "xgboost-1.7.4        | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libcblas-3.9.0       | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libcblas-3.9.0       | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python_abi-3.9       | 4 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scikit-learn-1.2.2   | 6.6 MB    | #############4                        |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "conda-23.7.2         | 995 KB    | 5                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "threadpoolctl-3.2.0  | 20 KB     | ############################8         |  78% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "scipy-1.10.1         | 19.5 MB   | ##########9                           |  30% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran-5.0.0    | 130 KB    | ####5                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python_abi-3.9       | 4 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "threadpoolctl-3.2.0  | 20 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scikit-learn-1.2.2   | 6.6 MB    | #################################2    |  90% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_py-xgboost-mutex-2. | 8 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.23   | 5.7 MB    | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgfortran-5.0.0    | 130 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "scipy-1.10.1         | 19.5 MB   | ################7                     |  45% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libxgboost-1.7.4     | 1.5 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libxgboost-1.7.4     | 1.5 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "numpy-1.23.5         | 5.3 MB    | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_py-xgboost-mutex-2. | 8 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "notebook-6.3.0       | 6.3 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "scipy-1.10.1         | 19.5 MB   | #####################6                |  58% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.23   | 5.7 MB    | #############1                        |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "numpy-1.23.5         | 5.3 MB    | ###############1                      |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "notebook-6.3.0       | 6.3 MB    | #############4                        |  36% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.23   | 5.7 MB    | #####################6                |  59% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "scipy-1.10.1         | 19.5 MB   | ##########################            |  70% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "numpy-1.23.5         | 5.3 MB    | #################################4    |  90% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "notebook-6.3.0       | 6.3 MB    | #############################         |  78% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.23   | 5.7 MB    | ##################################6   |  94% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "scipy-1.10.1         | 19.5 MB   | ##############################1       |  82% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "conda-23.7.2         | 995 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "conda-23.7.2         | 995 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libopenblas-0.3.23   | 5.7 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "scikit-learn-1.2.2   | 6.6 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "numpy-1.23.5         | 5.3 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "scipy-1.10.1         | 19.5 MB   | ##################################### | 100% \u001b[A\n",
      "scipy-1.10.1         | 19.5 MB   | ##################################### | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# conda install -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/songjiyeon/opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - matplotlib\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    brotli-1.0.9               |       hb7f2c08_9          20 KB  conda-forge\n",
      "    brotli-bin-1.0.9           |       hb7f2c08_9          18 KB  conda-forge\n",
      "    cycler-0.11.0              |     pyhd8ed1ab_0          10 KB  conda-forge\n",
      "    fonttools-4.41.1           |   py39hdc70f33_0         2.0 MB  conda-forge\n",
      "    kiwisolver-1.4.4           |   py39h92daf61_1          63 KB  conda-forge\n",
      "    libbrotlicommon-1.0.9      |       hb7f2c08_9          68 KB  conda-forge\n",
      "    libbrotlidec-1.0.9         |       hb7f2c08_9          31 KB  conda-forge\n",
      "    libbrotlienc-1.0.9         |       hb7f2c08_9         273 KB  conda-forge\n",
      "    matplotlib-3.5.3           |   py39h6e9494a_2           7 KB  conda-forge\n",
      "    matplotlib-base-3.5.3      |   py39h35e3ac7_2         7.4 MB  conda-forge\n",
      "    munkres-1.1.4              |     pyh9f0ad1d_0          12 KB  conda-forge\n",
      "    pyparsing-3.1.0            |     pyhd8ed1ab_0          87 KB  conda-forge\n",
      "    unicodedata2-15.0.0        |   py39ha30fb19_0         486 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        10.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  brotli             conda-forge/osx-64::brotli-1.0.9-hb7f2c08_9 \n",
      "  brotli-bin         conda-forge/osx-64::brotli-bin-1.0.9-hb7f2c08_9 \n",
      "  cycler             conda-forge/noarch::cycler-0.11.0-pyhd8ed1ab_0 \n",
      "  fonttools          conda-forge/osx-64::fonttools-4.41.1-py39hdc70f33_0 \n",
      "  kiwisolver         conda-forge/osx-64::kiwisolver-1.4.4-py39h92daf61_1 \n",
      "  libbrotlicommon    conda-forge/osx-64::libbrotlicommon-1.0.9-hb7f2c08_9 \n",
      "  libbrotlidec       conda-forge/osx-64::libbrotlidec-1.0.9-hb7f2c08_9 \n",
      "  libbrotlienc       conda-forge/osx-64::libbrotlienc-1.0.9-hb7f2c08_9 \n",
      "  matplotlib         conda-forge/osx-64::matplotlib-3.5.3-py39h6e9494a_2 \n",
      "  matplotlib-base    conda-forge/osx-64::matplotlib-base-3.5.3-py39h35e3ac7_2 \n",
      "  munkres            conda-forge/noarch::munkres-1.1.4-pyh9f0ad1d_0 \n",
      "  pyparsing          conda-forge/noarch::pyparsing-3.1.0-pyhd8ed1ab_0 \n",
      "  unicodedata2       conda-forge/osx-64::unicodedata2-15.0.0-py39ha30fb19_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "brotli-bin-1.0.9     | 18 KB     |                                       |   0% \n",
      "unicodedata2-15.0.0  | 486 KB    |                                       |   0% \u001b[A\n",
      "\n",
      "brotli-1.0.9         | 20 KB     |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libbrotlidec-1.0.9   | 31 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-base-3.5. | 7.4 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlienc-1.0.9   | 273 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kiwisolver-1.4.4     | 63 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fonttools-4.41.1     | 2.0 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlicommon-1.0. | 68 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cycler-0.11.0        | 10 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-3.5.3     | 7 KB      |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pyparsing-3.1.0      | 87 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "munkres-1.1.4        | 12 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libbrotlidec-1.0.9   | 31 KB     | ##################9                   |  51% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "brotli-bin-1.0.9     | 18 KB     | #################################7    |  91% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-base-3.5. | 7.4 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "unicodedata2-15.0.0  | 486 KB    | #2                                    |   3% \u001b[A\n",
      "\n",
      "\n",
      "brotli-bin-1.0.9     | 18 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "brotli-1.0.9         | 20 KB     | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlienc-1.0.9   | 273 KB    | ##1                                   |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kiwisolver-1.4.4     | 63 KB     | #########3                            |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fonttools-4.41.1     | 2.0 MB    | 2                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kiwisolver-1.4.4     | 63 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-base-3.5. | 7.4 MB    | #####2                                |  14% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlienc-1.0.9   | 273 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlicommon-1.0. | 68 KB     | ########7                             |  24% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cycler-0.11.0        | 10 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libbrotlicommon-1.0. | 68 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cycler-0.11.0        | 10 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "unicodedata2-15.0.0  | 486 KB    | ##################################### | 100% \u001b[A\n",
      "unicodedata2-15.0.0  | 486 KB    | ##################################### | 100% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fonttools-4.41.1     | 2.0 MB    | ####################2                 |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-3.5.3     | 7 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-3.5.3     | 7 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pyparsing-3.1.0      | 87 KB     | ######8                               |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "munkres-1.1.4        | 12 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "matplotlib-base-3.5. | 7.4 MB    | #############################5        |  80% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "munkres-1.1.4        | 12 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "pyparsing-3.1.0      | 87 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fonttools-4.41.1     | 2.0 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fonttools-4.41.1     | 2.0 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# conda install -c conda-forge matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true,
    "id": "OqnRjwHB1b64"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    make_scorer,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    average_precision_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "pd.set_option(\"display.max_columns\", 1000)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "xgboost.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "C5lJeTBu1b65"
   },
   "source": [
    "## 2. Ingest Data from links below and make a dataframe\n",
    "- Soldiers Male : https://query.data.world/s/h3pbhckz5ck4rc7qmt2wlknlnn7esr\n",
    "- Soldiers Female : https://query.data.world/s/sq27zz4hawg32yfxksqwijxmpwmynq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "hidden": true,
    "id": "tG5BsWraqX_y",
    "outputId": "a6e730fa-6848-4cfc-becb-284fc8cc99e5"
   },
   "outputs": [],
   "source": [
    "df_femal = pd.read_csv(\"ANSUR II FEMALE Public.csv\")\n",
    "df_femal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_male = pd.read_csv(\"ANSUR II MALE Public.csv\", encoding=\"ISO-8859-1\")\n",
    "df_male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_male = pd.read_csv(\"https://query.data.world/s/h3pbhckz5ck4rc7qmt2wlknlnn7esr\", encoding=\"latin-1\")\n",
    "\n",
    "# encoding = \"latin-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_female = pd.read_csv(\"https://query.data.world/s/sq27zz4hawg32yfxksqwijxmpwmynq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "TMjCTEG51b67"
   },
   "source": [
    "## 3. Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_male.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_female.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "hidden": true,
    "id": "SnlGRPWbrNAj",
    "outputId": "29ed8227-1450-4213-e9d1-a2953167d415"
   },
   "outputs": [],
   "source": [
    "# change the columns name.\n",
    "\n",
    "df_female.rename(columns={\"SubjectId\": \"subjectid\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_female.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_male.shape)\n",
    "print(df_female.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's concatenate female and male dataframes. \n",
    "\n",
    "\n",
    "df = pd.concat([df_male, df_female], axis=0, ignore_index=True)\n",
    "# ignore_index = True :   .\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(max_cols=110)\n",
    "\n",
    "# If we want to see the information of all features in dfs with\n",
    "# many features, we can set max_cols to a higher value.\n",
    "# info   columns      . max_cols    ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing values\n",
    "\n",
    "df.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking duplicated values\n",
    "\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"DODRace\"].value_counts())\n",
    "df[\"DODRace\"].value_counts().plot(kind=\"pie\", autopct=\"%1.1f%%\", figsize=(5, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = []\n",
    "for col in df:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        print(f\"{col} = {df[col].isnull().sum()}\")\n",
    "        drop_list.append(col)\n",
    "\n",
    "\n",
    "drop_list\n",
    "\n",
    "# We have identified the featurs with missing value. There are 4647\n",
    "# missing values in Ethnicity feature. As a best practice, if we have\n",
    "# a missisng value not more than 20% and 30% of the total number of observations,\n",
    "# these values can be filled. However, if it is more, the relevant data is\n",
    "# dropped as it will be data manipulation.\n",
    "# We will drop this feature since it corresponds to 4647/6068 = 76%.\n",
    "\n",
    "# You can select a value between 6068 x 0.2 = 1213 and 6068 x 0.3 = 1820 and\n",
    "# drop features with more missing values than this value.\n",
    "\n",
    "# As follows\n",
    "\n",
    "# drop_list =[]\n",
    "# for col in df:\n",
    "# if df[col].isnull().sum()>1800:\n",
    "# print(f\"{col} = {df[col].isnull().sum()}\")\n",
    "# drop_list.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=drop_list, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values after drop.\n",
    "\n",
    "df.isnull().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "\n",
    "# Our feature count dropped from 108 to 107."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find how many unique values object features have\n",
    "for col in df.select_dtypes(\"object\"):\n",
    "    print(f\"{col} has {df[col].nunique()} unique value\")\n",
    "\n",
    "# We check our unique categorical observation numbers.\n",
    "# We will drop the feature (Date), which shows the body measurement dates,\n",
    "# the units where the measurements are done (installation),\n",
    "# the specialty of the soldiers (PrimaryMOS) will not provide insight into races.\n",
    "\n",
    "# We will check below whether the unit (componenet) where the soldiers are working\n",
    "# and the branch (branch) they are working with have an effect.\n",
    "# (Like blacks with relatively better physical strength come to the fore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DODRace.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DODRace\"] = df.DODRace.map(\n",
    "    {\n",
    "        1: \"White\",\n",
    "        2: \"Black\",\n",
    "        3: \"Hispanic\",\n",
    "        4: \"Asian\",\n",
    "        5: \"Native American\",\n",
    "        6: \"Pacific Islander\",\n",
    "        8: \"Other\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# We rename the numeric data in my target according to the ethnic names\n",
    "# specified in the document to make it more understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Component\"])[\"DODRace\"].value_counts(normalize=True)\n",
    "\n",
    "# When we compare the distribution of the soldiers according to the units\n",
    "# they serve, the white, black, hispanic distribution in the 3 units is similar\n",
    "#  or close to the general distribution in the original data.\n",
    "# There is no component dominated by Hispanic race. We were unable to obtain\n",
    "# insight into the identification of race through Component feature.\n",
    "\n",
    "\n",
    "# army national guard, white = 68%\n",
    "# regular army, white = 58%...\n",
    "#  =  ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Component\"])[\"DODRace\"].value_counts().plot(kind=\"barh\", figsize=(7, 7))\n",
    "# pandas plot   bar plot    . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Component\", \"Branch\"])[\"DODRace\"].value_counts(normalize=True)\n",
    "\n",
    "# we obtained insights similar to the insights we obtained above.\n",
    "# There are no Hispanic-dominated components or branches.\n",
    "\n",
    "# We will drop \"Component\", \"Branch\" features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Component\", \"Branch\"])[\"DODRace\"].value_counts().plot(\n",
    "    kind=\"barh\", figsize=(7, 10)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.SubjectNumericRace.value_counts()\n",
    "\n",
    "# a feature describing our target DODRace.\n",
    "# We will drop it as it will cause data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list2 = [\n",
    "    \"Date\",\n",
    "    \"Installation\",\n",
    "    \"Component\",\n",
    "    \"Branch\",\n",
    "    \"PrimaryMOS\",\n",
    "    \"Weightlbs\",\n",
    "    \"Heightin\",\n",
    "    \"subjectid\",\n",
    "    \"SubjectNumericRace\",\n",
    "]\n",
    "\n",
    "df.drop(columns=drop_list2, inplace=True)\n",
    "\n",
    "#     drop\n",
    "\n",
    "#  object type .nunique()      .\n",
    "# Gender has 2 unique value\n",
    "# Date has 253 unique value -  unique values\n",
    "# Installation has 12 unique value\n",
    "# Component has 3 unique value\n",
    "# Branch has 3 unique value\n",
    "# PrimaryMOS has 285 unique value -  unique values\n",
    "# SubjectsBirthLocation has 152 unique value\n",
    "# WritingPreference has 3 unique value\n",
    "\n",
    "# We drop the \"Weightlbs\", \"Heightin\" features as they consist of information\n",
    "# that soldiers self-report.\n",
    "# height and weight information is measured and included in the data\n",
    "# as a separate feature.\n",
    "# Since subjectid is the unique registration number of the soldiers,\n",
    "# it will cause leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DODRace.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df[\"DODRace\"].isin([\"White\", \"Black\", \"Hispanic\"])]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe().T\n",
    "\n",
    "# we have 98 columns, this is not easy for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade pandas\n",
    "\n",
    "# update ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(df2.corr(numeric_only=True), vmin=-1, vmax=1, cmap=\"coolwarm\");\n",
    "\n",
    "# has a multicollinearity issue, but it's fine for logregg with regularisation and non-parametric algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "CS5-GZy0sl4s"
   },
   "source": [
    "# DATA Preprocessing\n",
    "- In this step we divide our data to X(Features) and y(Target) then ,\n",
    "- To train and evaluation purposes we create train and test sets,\n",
    "- Lastly, scale our data if features not in same scale. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "fr2wgpvk1b7B"
   },
   "outputs": [],
   "source": [
    "# X (Features)= independent\n",
    "# y (Target)= dependent\n",
    "\n",
    "\n",
    "X = df2.drop(columns=[\"DODRace\"])\n",
    "y = df2.DODRace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=101, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train features shape : \", X_train.shape)\n",
    "print(\"Train target shape   : \", y_train.shape)\n",
    "print(\"Test features shape  : \", X_test.shape)\n",
    "print(\"Test target shape    : \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "zfi_NOw0s2fM"
   },
   "source": [
    "# Modelling\n",
    "- Fit the model with train dataset\n",
    "- Get predict from vanilla model on both train and test sets to examine if there is over/underfitting   \n",
    "- Apply GridseachCV for both hyperparemeter tuning and sanity test of our model.\n",
    "- Use hyperparameters that you find from gridsearch and make final prediction and evaluate the result according to chosen metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "N1cviBuh1b7C"
   },
   "source": [
    "## 1. Logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "0rSJ5hxp1b7C"
   },
   "source": [
    "### Vanilla Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rbKDDck012BS"
   },
   "outputs": [],
   "source": [
    "def eval_metric(model, X_train, y_train, X_test, y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"Test_Set\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print()\n",
    "    print(\"Train_Set\")\n",
    "    print(confusion_matrix(y_train, y_train_pred))\n",
    "    print(classification_report(y_train, y_train_pred))\n",
    "    \n",
    "    # this function reutrns summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = X_train.select_dtypes(\"object\").columns\n",
    "cat\n",
    "\n",
    "# cat = categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "column_trans = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat),\n",
    "    remainder=MinMaxScaler(),\n",
    "    verbose_feature_names_out=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\n",
    "        \"log\",\n",
    "        LogisticRegression(class_weight=\"balanced\", max_iter=10000, random_state=101),\n",
    "    ),\n",
    "]\n",
    "\n",
    "pipe_log_model = Pipeline(steps=operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_log_model.fit(X_train, y_train)\n",
    "eval_metric(pipe_log_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {\n",
    "    \"precision_Hispanic\": make_scorer(\n",
    "        precision_score, average=None, labels=[\"Hispanic\"]\n",
    "    ),\n",
    "    \"recall_Hispanic\": make_scorer(recall_score, average=None, labels=[\"Hispanic\"]),\n",
    "    \"f1_Hispanic\": make_scorer(f1_score, average=None, labels=[\"Hispanic\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\n",
    "        \"log\",\n",
    "        LogisticRegression(class_weight=\"balanced\", max_iter=10000, random_state=101),\n",
    "    ),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations)\n",
    "\n",
    "scores = cross_validate(\n",
    "    model, X_train, y_train, scoring=scoring, cv=10, n_jobs=-1, return_train_score=True\n",
    ")\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 11))\n",
    "df_scores.mean()[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "lPelWxsU1b7C"
   },
   "source": [
    "### Logistic Model GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNZyqeNY15nP"
   },
   "outputs": [],
   "source": [
    "recall_Hispanic = make_scorer(recall_score, average=None, labels=[\"Hispanic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"log__C\": [0.5, 1],\n",
    "    \"log__penalty\": [\"l1\", \"l2\"],\n",
    "    \"log__solver\": [\"liblinear\", \"lbfgs\"], \n",
    "    # ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\n",
    "        \"log\",\n",
    "        LogisticRegression(class_weight=\"balanced\", max_iter=10000, random_state=101),\n",
    "    ),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations)\n",
    "\n",
    "\n",
    "log_model_grid = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring=recall_Hispanic,\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(log_model_grid.cv_results_).loc[\n",
    "    log_model_grid.best_index_, [\"mean_test_score\", \"mean_train_score\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(log_model_grid, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.metrics import plot_roc, plot_precision_recall\n",
    "\n",
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\n",
    "        \"log\",\n",
    "        LogisticRegression(class_weight=\"balanced\", max_iter=10000, random_state=101),\n",
    "    ),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "plot_precision_recall(y_test, y_pred_proba)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(y_test).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "y_test_dummies = pd.get_dummies(y_test).values\n",
    "\n",
    "average_precision_score(y_test_dummies[:, 1], y_pred_proba[:, 1])\n",
    "\n",
    "# Returns 0 black, 1 hispanic, 2 white scores.\n",
    "# We got hispanic scores by specifying 1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_model_grid.predict(X_test)\n",
    "\n",
    "log_AP = average_precision_score(y_test_dummies[:, 1], y_pred_proba[:, 1])\n",
    "log_f1 = f1_score(y_test, y_pred, average=None, labels=[\"Hispanic\"])\n",
    "log_recall = recall_score(y_test, y_pred, average=None, labels=[\"Hispanic\"])\n",
    "\n",
    "# Since we will compare the scores we got from all models in the table below,\n",
    "# we assign model scores to the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression solver \"liblenear\" for small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\n",
    "        \"log\",\n",
    "        LogisticRegression(\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=10000,\n",
    "            random_state=101,\n",
    "            solver=\"liblinear\",\n",
    "            penalty=\"l1\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "pipe_log_model_libl = Pipeline(steps=operations)\n",
    "\n",
    "# Since it is stated in the logistic regression documentation that liblinear is\n",
    "# a good choice as a solver for small datasets.\n",
    "# We are using liblinear. It can be used with liblinear l2 and l1.\n",
    "# but we preferred l1 as it gives better scores with l1 than l2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_log_model_libl.fit(X_train, y_train)\n",
    "eval_metric(pipe_log_model_libl, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\n",
    "        \"log\",\n",
    "        LogisticRegression(\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=10000,\n",
    "            random_state=101,\n",
    "            solver=\"liblinear\",\n",
    "            penalty=\"l1\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations)\n",
    "\n",
    "scores = cross_validate(\n",
    "    model, X_train, y_train, scoring=scoring, cv=10, n_jobs=-1, return_train_score=True\n",
    ")\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 11))\n",
    "df_scores.mean()[2:]\n",
    "\n",
    "# just 1 point better than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "GM0PL5eZ1b7E"
   },
   "source": [
    "## 2. SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "P3j_Xk1L1b7E"
   },
   "source": [
    "### Vanilla SVC model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pro8T6CM19vX"
   },
   "outputs": [],
   "source": [
    "operations_svc = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\"svc\", SVC(class_weight=\"balanced\", random_state=101)),\n",
    "]\n",
    "\n",
    "pipe_svc_model = Pipeline(steps=operations_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dFocgCo1-0Z"
   },
   "outputs": [],
   "source": [
    "pipe_svc_model.fit(X_train, y_train)\n",
    "\n",
    "eval_metric(pipe_svc_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=operations_svc)\n",
    "\n",
    "scores = cross_validate(\n",
    "    model, X_train, y_train, scoring=scoring, cv=10, n_jobs=-1, return_train_score=True\n",
    ")\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 11))\n",
    "df_scores.mean()[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "28y9nWxG1b7E"
   },
   "source": [
    "###  SVC Model GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"svc__C\": [0.5, 1], \"svc__gamma\": [\"scale\", \"auto\", 0.01]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations_svc = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\"svc\", SVC(class_weight=\"balanced\", random_state=101)),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations_svc)\n",
    "\n",
    "svm_model_grid = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring=recall_Hispanic,\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(svm_model_grid.cv_results_).loc[\n",
    "    svm_model_grid.best_index_, [\"mean_test_score\", \"mean_train_score\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(svm_model_grid, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m * (m-1) /2 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\"svc\", SVC(C=1, class_weight=\"balanced\", random_state=101)),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "decision_function = model.decision_function(X_test)\n",
    "\n",
    "# y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "plot_precision_recall(y_test, decision_function)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(y_test_dummies[:, 1], decision_function[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_model_grid.predict(X_test)\n",
    "\n",
    "svc_AP = average_precision_score(y_test_dummies[:, 1], decision_function[:, 1])\n",
    "svc_f1 = f1_score(y_test, y_pred, average=None, labels=[\"Hispanic\"])\n",
    "svc_recall = recall_score(y_test, y_pred, average=None, labels=[\"Hispanic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "bDX_iLIls74C"
   },
   "source": [
    "## 3. RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "ord_enc = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "\n",
    "column_trans = make_column_transformer((ord_enc, cat), remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "qaTzrT6P1b7G"
   },
   "source": [
    "### Vanilla RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvtqL0Qg2CKf"
   },
   "outputs": [],
   "source": [
    "operations_rf = [\n",
    "    (\"OrdinalEncoder\", column_trans),\n",
    "    (\"RF_model\", RandomForestClassifier(class_weight=\"balanced\", random_state=101)),\n",
    "]\n",
    "\n",
    "pipe_model_rf = Pipeline(steps=operations_rf)\n",
    "\n",
    "pipe_model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(pipe_model_rf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# we can see it's very bad at Test_Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations_rf = [\n",
    "    (\"OrdinalEncoder\", column_trans),\n",
    "    (\"RF_model\", RandomForestClassifier(class_weight=\"balanced\", random_state=101)),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations_rf)\n",
    "\n",
    "scores = cross_validate(\n",
    "    model, X_train, y_train, scoring=scoring, cv=5, n_jobs=-1, return_train_score=True\n",
    ")\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 6))\n",
    "df_scores.mean()[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "MkLAZ_M41b7G"
   },
   "source": [
    "### RF Model GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpmPr3202EbD"
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"RF_model__n_estimators\": [400, 500],\n",
    "    \"RF_model__max_depth\": [2, 3],\n",
    "    # 'RF_model__min_samples_split':[18,20,22],\n",
    "    # 'RF_model__max_features': ['auto', None, 15, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations_rf = [\n",
    "    (\"OrdinalEncoder\", column_trans),\n",
    "    (\"RF_model\", RandomForestClassifier(class_weight=\"balanced\", random_state=101)),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations_rf)\n",
    "rf_grid_model = GridSearchCV(\n",
    "    model, param_grid, scoring=recall_Hispanic, n_jobs=-1, return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rf_grid_model.cv_results_).loc[\n",
    "    rf_grid_model.best_index_, [\"mean_test_score\", \"mean_train_score\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(rf_grid_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations_rf = [\n",
    "    (\"OrdinalEncoder\", column_trans),\n",
    "    (\n",
    "        \"RF_model\",\n",
    "        RandomForestClassifier(\n",
    "            class_weight=\"balanced\", max_depth=2, n_estimators=400, random_state=101\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations_rf)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "plot_precision_recall(y_test, y_pred_proba)\n",
    "plt.show();\n",
    "\n",
    "# overall, this model is not as good as the other model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(y_test_dummies[:, 1], y_pred_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_grid_model.predict(X_test)\n",
    "\n",
    "rf_AP = average_precision_score(y_test_dummies[:, 1], y_pred_proba[:, 1])\n",
    "rf_f1 = f1_score(y_test, y_pred, average=None, labels=[\"Hispanic\"])\n",
    "rf_recall = recall_score(y_test, y_pred, average=None, labels=[\"Hispanic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "PvcPc4V81b7H"
   },
   "source": [
    "## 4. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "MuxxUFoW1b7H"
   },
   "source": [
    "### Vanilla XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nfi1aa152HbR"
   },
   "outputs": [],
   "source": [
    "# Random Forest (DT) = bagging.\n",
    "# XGBoost = boosting.\n",
    "\n",
    "operations_xgb = [\n",
    "    (\"OrdinalEncoder\", column_trans),\n",
    "    (\"XGB_model\", XGBClassifier(random_state=101)),\n",
    "]\n",
    "\n",
    "pipe_model_xgb = Pipeline(steps=operations_xgb)\n",
    "# sorting will be same as classification_report.\n",
    "y_train_xgb = y_train.map({\"Black\": 0, \"Hispanic\": 1, \"White\": 2})\n",
    "y_test_xgb = y_test.map({\"Black\": 0, \"Hispanic\": 1, \"White\": 2})\n",
    "# If the target is not numeric in xgb 1.6 and higher versions, it returns an error.\n",
    "# That's why we do the conversion manually.\n",
    "\n",
    "\n",
    "pipe_model_xgb.fit(X_train, y_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(pipe_model_xgb, X_train, y_train_xgb, X_test, y_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "classes_weights = class_weight.compute_sample_weight(\n",
    "    class_weight=\"balanced\", y=y_train_xgb\n",
    ")\n",
    "classes_weights\n",
    "\n",
    "# XGboost algorithm has no class_weight hyperparameter for multiclass.\n",
    "# We can use the sample_weight hyperparameter within the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {\"weights\": classes_weights, \"label\": y_train_xgb}\n",
    "\n",
    "comp = pd.DataFrame(my_dict)\n",
    "\n",
    "comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp.groupby(\"label\").value_counts()\n",
    "\n",
    "# weight x class will give us the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_model_xgb.fit(X_train, y_train_xgb, XGB_model__sample_weight=classes_weights)\n",
    "\n",
    "\n",
    "# weight parameter in XGBoost is per instance not per class. Therefore,\n",
    "# we need to assign the weight of each class to its instances,\n",
    "# which is the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(pipe_model_xgb, X_train, y_train_xgb, X_test, y_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_xgb = {\n",
    "    \"precision_Hispanic\": make_scorer(precision_score, average=None, labels=[1]),\n",
    "    \"recall_Hispanic\": make_scorer(recall_score, average=None, labels=[1]),\n",
    "    \"f1_Hispanic\": make_scorer(f1_score, average=None, labels=[1]),\n",
    "}\n",
    "\n",
    "# Since xgb does not accept non-encoded targets, we arrange the scoring_xgb\n",
    "# according to the 1 numeric class corresponding to hispanics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations_xgb = [\n",
    "    (\"OrdinalEncoder\", column_trans),\n",
    "    (\"XGB_model\", XGBClassifier(random_state=101)),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations_xgb)\n",
    "\n",
    "scores = cross_validate(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train_xgb,\n",
    "    scoring=scoring_xgb,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    "    fit_params={\"XGB_model__sample_weight\": classes_weights},\n",
    ")\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 6))\n",
    "df_scores.mean()[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "p3gH5QvE1b7I"
   },
   "source": [
    "### XGBoost Model GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72E3Cmnm2KOE"
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"XGB_model__n_estimators\": [20, 40],\n",
    "    \"XGB_model__max_depth\": [1, 2],\n",
    "    \"XGB_model__learning_rate\": [0.03, 0.05],\n",
    "    \"XGB_model__subsample\": [0.8, 1],\n",
    "    \"XGB_model__colsample_bytree\": [0.8, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations_xgb = [\n",
    "    (\"OrdinalEncoder\", column_trans),\n",
    "    (\"XGB_model\", XGBClassifier(random_state=101)),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations_xgb)\n",
    "\n",
    "xgb_grid_model = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(recall_score, average=None, labels=[1]),\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid_model.fit(X_train, y_train_xgb, XGB_model__sample_weight=classes_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(xgb_grid_model.cv_results_).loc[\n",
    "    xgb_grid_model.best_index_, [\"mean_test_score\", \"mean_train_score\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(xgb_grid_model, X_train, y_train_xgb, X_test, y_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.metrics import plot_roc, precision_recall_curve\n",
    "\n",
    "\n",
    "operations_xgb = [\n",
    "    (\"OrdinalEncoder\", column_trans),\n",
    "    (\n",
    "        \"XGB_model\",\n",
    "        XGBClassifier(\n",
    "            colsample_bytree=0.8,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=2,\n",
    "            n_estimators=40,\n",
    "            subsample=0.8,\n",
    "            random_state=101,\n",
    "        ),# based on best_params\n",
    "    ),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations_xgb)\n",
    "\n",
    "model.fit(X_train, y_train_xgb, XGB_model__sample_weight=classes_weights)\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "plot_precision_recall(y_test_xgb, y_pred_proba)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_xgb_dummies = pd.get_dummies(y_test_xgb).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(y_test_xgb_dummies[:, 1], y_pred_proba[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_grid_model.predict(X_test)\n",
    "\n",
    "xgb_AP = average_precision_score(y_test_xgb_dummies[:, 1], y_pred_proba[:, 1])\n",
    "xgb_f1 = f1_score(y_test_xgb, y_pred, average=None, labels=[1])\n",
    "xgb_recall = recall_score(y_test_xgb, y_pred, average=None, labels=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "DbXAmOPVDatl"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"Logistic Regression\", \"SVM\", \"Random Forest\", \"XGBoost\"],\n",
    "        \"F1\": [log_f1[0], svc_f1[0], rf_f1[0], xgb_f1[0]],\n",
    "        \"Recall\": [log_recall[0], svc_recall[0], rf_recall[0], xgb_recall[0]],\n",
    "        \"AP\": [log_AP, svc_AP, rf_AP, xgb_AP],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.subplot(311)\n",
    "compare = compare.sort_values(by=\"F1\", ascending=False)\n",
    "ax = sns.barplot(x=\"F1\", y=\"Model\", data=compare, palette=\"Blues_d\")\n",
    "ax.bar_label(ax.containers[0], fmt=\"%.3f\")\n",
    "\n",
    "plt.subplot(312)\n",
    "compare = compare.sort_values(by=\"Recall\", ascending=False)\n",
    "ax = sns.barplot(x=\"Recall\", y=\"Model\", data=compare, palette=\"Blues_d\")\n",
    "ax.bar_label(ax.containers[0], fmt=\"%.3f\")\n",
    "\n",
    "plt.subplot(313)\n",
    "compare = compare.sort_values(by=\"AP\", ascending=False)\n",
    "ax = sns.barplot(x=\"AP\", y=\"Model\", data=compare, palette=\"Blues_d\")\n",
    "ax.bar_label(ax.containers[0], fmt=\"%.3f\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "WvWpInu21b7L"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "xg2k1ScZ1b7L"
   },
   "source": [
    "# SMOTE\n",
    "https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "9Rqk02x61b7L"
   },
   "source": [
    "##  Smote implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "XTN4iO7i1b7L"
   },
   "outputs": [],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda update -n base -c defaults conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Cv5155AN1b7L"
   },
   "outputs": [],
   "source": [
    "# equalizes or approximates minority classes to the majority class.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# equalizes or converges the majority class to the minority class\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "# imblearn.pipeline is different from the pipeline library of the sklearn library.\n",
    "# While the sklearn pipeline only runs the fit, transform, predict and predict_proba functions,\n",
    "# imblearn.pipeline also runs the resample function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1FSKgtbaylV"
   },
   "outputs": [],
   "source": [
    "column_trans = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat),\n",
    "    remainder=MinMaxScaler(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ohe = column_trans.fit_transform(X_train)\n",
    "\n",
    "# First, we manually transform X_train and X_test' one hot encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over = SMOTE()\n",
    "X_train_over, y_train_over = over.fit_resample(X_train_ohe, y_train)\n",
    "\n",
    "# over_sampling(smote) and under_sampling (RandomUnderSampler) apply to X_train and y_train only.\n",
    "# If it is applied to all the data and then separated into data train and test set,\n",
    "# data_leakage will occur and your scores will overestimated.\n",
    "# The data should be split into train and test set first and then applied only to the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_over.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_over.value_counts()\n",
    "\n",
    "# As you can see, all classes after smote are equal to the number of\n",
    "# observations of the majority class, white.\n",
    "\n",
    "# However, equating or approximating the minority class to the majority\n",
    "# class will result in overfitting of the model. Therefore, the number of observations\n",
    "# belonging to the minority class should be increased in a controlled manner\n",
    "# to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under = RandomUnderSampler()\n",
    "X_train_under, y_train_under = under.fit_resample(X_train_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_under.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_under.value_counts()\n",
    "\n",
    "# As you can see, all classes after RandomUnderSampler are equal to the\n",
    "# number of observations in the minority class hispanic.\n",
    "\n",
    "# Equalizing or approximating the majority class to the minority class will\n",
    "# result in a very serious loss of information of the observations in\n",
    "# the majority class. Even if the scores improve, this improvement will only be\n",
    "# an overestimate. For this reason, a 20%-30% reduction should be done from\n",
    "# the Majority class. However, if there is still no improvement in the scores,\n",
    "# this rate should not be increased further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over = SMOTE(sampling_strategy={\"Hispanic\": 1000})\n",
    "under = RandomUnderSampler(sampling_strategy={\"White\": 2500})\n",
    "\n",
    "# With the sampling_strategy hyperparameter, we can increase or decrease\n",
    "# the classes however we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled_over, y_resampled_over = over.fit_resample(X_train_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resampled_over.value_counts()\n",
    "\n",
    "# With over, we doubled the number of observations in the Hispanic class.\n",
    "# But we need to check below whether this increase causes overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled_under, y_resampled_under = under.fit_resample(X_train_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resampled_under.value_counts()\n",
    "# We reduced the number of white observations by around 18% and equalized to 2500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We automate our over_sampling and under_sampling processes.\n",
    "steps = [(\"o\", over), (\"u\", under)]\n",
    "\n",
    "\n",
    "pipeline = imbpipeline(steps=steps)\n",
    "\n",
    "# First the number of hispanic cals will be increased to 1000 as per our instruction,\n",
    "# and then the white class will be reduced to 2500.\n",
    "X_resampled, y_resampled = pipeline.fit_resample(X_train_ohe, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "1hBIqmFL1b7O"
   },
   "source": [
    "## Logistic Regression Over/ Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12AItu1M2ds0"
   },
   "outputs": [],
   "source": [
    "column_trans = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat),\n",
    "    remainder=MinMaxScaler(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLsqCpGn7jNM"
   },
   "outputs": [],
   "source": [
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans),\n",
    "    (\"o\", over),\n",
    "    (\"u\", under),\n",
    "    (\"log\", LogisticRegression(max_iter=10000, random_state=101)),\n",
    "]  # (\"scaler\", MinMaxScaler())\n",
    "\n",
    "# When over or under sampling is applied to data, class_weight=\"balanced\" is never used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_pipeline = imbpipeline(steps=operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(smote_pipeline, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = imbpipeline(steps=operations)\n",
    "\n",
    "scores = cross_validate(\n",
    "    model, X_train, y_train, scoring=scoring, cv=10, n_jobs=-1, return_train_score=True\n",
    ")\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 11))\n",
    "df_scores.mean()[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9A1B65L7jwp"
   },
   "source": [
    "## Other Evaluation Metrics for Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "T1pPLjpA1b7P"
   },
   "source": [
    "- Evaluation metrics \n",
    "https://towardsdatascience.com/comprehensive-guide-on-multiclass-classification-metrics-af94cfb83fbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "Me3OZtQF1b7P",
    "outputId": "6280fddb-b392-40c1-87d3-57d242ce2528"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_corrcoef?\n",
    "matthews_corrcoef(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "P74oLhzK1b7P",
    "outputId": "350b8e9d-dd72-4566-8b11-10526699cd94"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "cohen_kappa_score?\n",
    "cohen_kappa_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "9hxUcvZG1b7J"
   },
   "source": [
    "# Before the Deployment \n",
    "- Choose the model that works best based on your chosen metric\n",
    "- For final step, fit the best model with whole dataset to get better performance.\n",
    "- And your model ready to deploy, dump your model and scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UOn_G0n2N2Z"
   },
   "outputs": [],
   "source": [
    "column_trans_final = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat),\n",
    "    remainder=MinMaxScaler(),\n",
    ")\n",
    "\n",
    "operations_final = [\n",
    "    (\"OneHotEncoder\", column_trans_final),\n",
    "    (\n",
    "        \"log\",\n",
    "        LogisticRegression(class_weight=\"balanced\", max_iter=10000, random_state=101),\n",
    "    ),\n",
    "]\n",
    "\n",
    "final_model = Pipeline(steps=operations_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X.Gender == \"Male\"].describe()\n",
    "\n",
    "# We will make predictions on the model based on the average\n",
    "# values of all male soldiers in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_mean_human = X[X.Gender == \"Male\"].describe(include=\"all\").loc[\"mean\"]\n",
    "male_mean_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_mean_human[\"Gender\"] = \"Male\"\n",
    "male_mean_human[\"SubjectsBirthLocation\"] = \"California\"\n",
    "male_mean_human[\"WritingPreference\"] = \"Right hand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(male_mean_human).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.predict(pd.DataFrame(male_mean_human).T)\n",
    "\n",
    "# we can say that the average values of male soldiers are very\n",
    "# close to Hispanic soldiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# Accuracy is an unreliable metric for unstable datasets. For this reason,\n",
    "# matthews_corrcoef and cohen_kappa_score can be used for accuracy of\n",
    "# imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "cohen_kappa_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows how much all the featurs in our Shap data contribute to predicting classes.\n",
    "\n",
    "Before obtaining Shap scores, the model that gives us the best score must be selected. Since we got the best score in logistic regression above, we will continue with logistic regression. However, since we will make a feature selection, we need to make the penalty hyper_parameter l1 (lasso).\n",
    "\n",
    "Since l1 can be used with saga, sagaj and liblnear, we should go with one of these solvers. We will continue with solver=right, since the solver to the right gives the best score with l1.\n",
    "\n",
    "Since it does not work with the model installed with the shap pipeline, we will do the conversions manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans_shap = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat),\n",
    "    remainder=MinMaxScaler(),\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "X_train_trans = column_trans_shap.fit_transform(X_train)\n",
    "X_test_trans = column_trans_shap.transform(X_test)\n",
    "\n",
    "model_shap = LogisticRegression(\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=10000,\n",
    "    random_state=101,\n",
    "    penalty=\"l1\",\n",
    "    solver=\"saga\",\n",
    ")\n",
    "\n",
    "model_shap.fit(X_train_trans, y_train)\n",
    "\n",
    "# Since the Shap doesn't work with the model fitted with pipeline,\n",
    "# we will apply the conversions manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(model_shap, X_train_trans, y_train, X_test_trans, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations = [\n",
    "    (\"OneHotEncoder\", column_trans_shap),\n",
    "    (\n",
    "        \"log\",\n",
    "        LogisticRegression(\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=10000,\n",
    "            random_state=101,\n",
    "            penalty=\"l1\",\n",
    "            solver=\"saga\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "model = Pipeline(steps=operations)\n",
    "\n",
    "scores = cross_validate(\n",
    "    model, X_train, y_train, scoring=scoring, cv=5, n_jobs=-1, return_train_score=True\n",
    ")\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 6))\n",
    "df_scores.mean()[2:]\n",
    "\n",
    "# no overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = column_trans_shap.get_feature_names_out()\n",
    "features\n",
    "\n",
    "# Since we perform the transformation with the make_column_transform function,\n",
    "# we will use this feature order within the Shap function since the feature\n",
    "# order of the X_train_trans and X_test_trans data changes as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap values for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.LinearExplainer(model_shap, X_train_trans)  # LinearExplainer\n",
    "\n",
    "shap_values = explainer.shap_values(X_test_trans)\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values, max_display=300, feature_names=features, plot_size=(20, 100)\n",
    ")\n",
    "\n",
    "# shap asks for numpy 1.24 or lower.\n",
    "\n",
    "# Since the shap values we get from X_test data may cause data_leakage.\n",
    "# We will get the shap values from the train data below.\n",
    "\n",
    "# Although it is recommended to use shap mostly with linear and treebased models.\n",
    "# You can get shap values in non-parametric models such as KNN and SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP values for Feature Selection (train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model_shap, X_train_trans)\n",
    "\n",
    "shap_values = explainer.shap_values(X_train_trans)\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values, max_display=300, feature_names=features, plot_size=(20, 100)\n",
    ")\n",
    "\n",
    "# blues black, pink white and green hispanic\n",
    "# as you can see, there is no feature that contributes to the prediction of hispanic among\n",
    "# the top 13 most important features.\n",
    "# We choose 19 features that contribute to the predictions of hispanics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hispanic = [\n",
    "    \"SubjectsBirthLocation\",\n",
    "    \"handlength\",\n",
    "    \"forearmcircumferenceflexed\",\n",
    "    \"headlength\",\n",
    "    \"bitragionsubmandibulararc\",\n",
    "    \"footbreadthhorizontal\",\n",
    "    \"bimalleolarbreadth\",\n",
    "    \"poplitealheight\",\n",
    "    \"waistdepth\",\n",
    "    \"WritingPreference\",\n",
    "    \"tragiontopofhead\",\n",
    "    \"bideltoidbreadth\",\n",
    "    \"neckcircumferencebase\",\n",
    "    \"biacromialbreadth\",\n",
    "    \"buttockheight\",\n",
    "    \"buttockkneelength\",\n",
    "    \"earlength\",\n",
    "    \"axillaheight\",\n",
    "    \"Age\",\n",
    "]\n",
    "\n",
    "# for features contributed hispanic class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X[hispanic]\n",
    "X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_new = X2.select_dtypes(\"object\").columns\n",
    "cat_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
    "    X2, y, test_size=0.2, random_state=101, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part is same as previous\n",
    "column_trans_shap = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_new),\n",
    "    remainder=MinMaxScaler(),\n",
    "    verbose_feature_names_out=False,\n",
    ")\n",
    "\n",
    "operations_shap = [\n",
    "    (\"OneHotEncoder\", column_trans_shap),\n",
    "    (\n",
    "        \"log\",\n",
    "        LogisticRegression(\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=10000,\n",
    "            random_state=101,\n",
    "            penalty=\"l1\",\n",
    "            solver=\"saga\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "pipe_shap_model = Pipeline(steps=operations_shap)\n",
    "pipe_shap_model.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric(pipe_shap_model, X_train2, y_train2, X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=operations_shap)\n",
    "\n",
    "scores = cross_validate(\n",
    "    model, X_train2, y_train2, scoring=scoring, cv=5, n_jobs=-1, return_train_score=True\n",
    ")\n",
    "df_scores = pd.DataFrame(scores, index=range(1, 6))\n",
    "df_scores.mean()[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = pipe_shap_model.predict_proba(X_test2)\n",
    "\n",
    "plot_precision_recall(y_test2, y_pred_proba)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "id": "g8_x-BiN1b7Q"
   },
   "source": [
    "___\n",
    "\n",
    "<p style=\"text-align: center;\"><img src=\"https://docs.google.com/uc?id=1lY0Uj5R04yMY3-ZppPWxqCr5pvBLYPnV\" class=\"img-fluid\" alt=\"CLRSWY\"></p>\n",
    "\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "K7UZHtvu1b62",
    "C5lJeTBu1b65",
    "TMjCTEG51b67",
    "CS5-GZy0sl4s",
    "zfi_NOw0s2fM",
    "p3gH5QvE1b7I",
    "xg2k1ScZ1b7L",
    "1hBIqmFL1b7O",
    "j9A1B65L7jwp",
    "9hxUcvZG1b7J"
   ],
   "provenance": []
  },
  "hide_input": false,
  "interpreter": {
   "hash": "e4e90950cb561445fc7289d5187c528b28750a487d008a70b474c773afaf79b7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "338.797px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 527,
   "position": {
    "height": "40px",
    "left": "1034px",
    "right": "20px",
    "top": "185px",
    "width": "661px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
